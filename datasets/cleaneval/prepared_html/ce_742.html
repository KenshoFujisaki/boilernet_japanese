<html>
 <body bgcolor="#FFFFFF" link="#0000FF" vlink="#800080">
  <text encoding="iso-8859-1" id="http://www.testing.com/writings/status/status.html" title="The Test Manager at the Project Status Meeting">
   <title>
    <span __boilernet_label="0">
     &lt;span __num="0"&gt;     The Test Manager at the Project Status Meeting    &lt;/span&gt;
    </span>
   </title>
   <table bgcolor="#CCCCCC" border="0" cellpadding="3" cellspacing="0" width="100%">
    <form action="http://www.google.com/search" method="get">
    </form>
    <tbody>
     <tr>
      <td width="65%">
       <p>
        <a href="http://www.testing.com">
         <span __boilernet_label="0" __num="1">
          testing.com
         </span>
        </a>
        <span __boilernet_label="0" __num="2">
         &gt;
        </span>
        <a href="../../writings.html">
         <span __boilernet_label="0" __num="3">
          Writings
         </span>
        </a>
        <span __boilernet_label="0" __num="4">
         &gt; Test Manager at theStatus Meeting
        </span>
       </p>
      </td>
      <td align="right" width="35%">
      </td>
     </tr>
    </tbody>
   </table>
   <table bgcolor="#AAAAAA" border="0" cellpadding="7" cellspacing="0" width="100%">
    <tbody>
     <tr>
      <td>
       <p align="center">
        <font size="5">
         <strong>
          <span __boilernet_label="0" __num="5">
           Testing Foundations
          </span>
         </strong>
        </font>
        <br/>
        <font size="2">
         <strong>
          <span __boilernet_label="0" __num="6">
           Consulting in Software Testing
          </span>
         </strong>
        </font>
        <br/>
        <a href="mailto:marick@testing.com" title="marick@testing.com">
         <span __boilernet_label="0" __num="7">
          Brian Marick
         </span>
        </a>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <table bgcolor="#CCCCCC" border="0" cellspacing="0" width="100%">
    <tbody>
     <tr>
      <td width="11%">
       <p align="center">
        <a href="/services.html" title="Consulting, training, and contracting services">
         <span __boilernet_label="0" __num="8">
          Services
         </span>
        </a>
       </p>
      </td>
      <td width="11%">
       <p align="center">
        <a href="/writings.html" title="Papers, essays, and book reviews">
         <span __boilernet_label="0" __num="9">
          Writings
         </span>
        </a>
       </p>
      </td>
      <td width="9%">
       <p align="center">
        <a href="/tools.html" title="Freeware tools I have written">
         <span __boilernet_label="0" __num="10">
          Tools
         </span>
        </a>
       </p>
      </td>
      <td width="17%">
       <p align="center">
        <a href="/agile/index.html" title="Agile Testing">
         <span __boilernet_label="0" __num="11">
          Agile Testing
         </span>
        </a>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p>
   </p>
   <p align="center">
    <font face="Arial,HELVETICA" size="6">
     <b>
      <span __boilernet_label="1">
       TheTest Manager at the Project Status Meeting
      </span>
     </b>
    </font>
   </p>
   <p align="center">
    <b>
     <span __boilernet_label="1">
      Brian Marick,
     </span>
    </b>
    <a href="http://www.testing.com">
     <span __boilernet_label="1">
      Testing Foundations
     </span>
    </a>
    <b>
    </b>
    <span __boilernet_label="1">
     (
    </span>
    <a href="mailto:marick@testing.com">
     <span __boilernet_label="1">
      marick@testing.com
     </span>
    </a>
    <span __boilernet_label="1">
     )
    </span>
    <b>
     <br/>
     <span __boilernet_label="1">
      Steve Stukenborg,
     </span>
    </b>
    <a href="http://www.pureatria.com/">
     <span __boilernet_label="1">
      PureAtria
     </span>
    </a>
    <b>
    </b>
    <span __boilernet_label="1">
     (now
    </span>
    <a href="http://www.rational.com">
     <span __boilernet_label="1">
      Rational
     </span>
    </a>
    <span __boilernet_label="1">
     )
    </span>
    <b>
     <br/>
     <span __boilernet_label="0" __num="23">
      Copyright
     </span>
    </b>
    <font face="Symbol">
     <b>
      <span __boilernet_label="0" __num="24">
       ï¿½
      </span>
     </b>
    </font>
    <b>
     <span __boilernet_label="0" __num="25">
      1997 byBrian Marick and Pure Atria Corporation. All Rights Reserved.
     </span>
    </b>
   </p>
   <p align="center">
    <a href="status.pdf">
     <span __boilernet_label="0" __num="26">
      A PDF version
     </span>
    </a>
    <br/>
    <a href="discuss.html">
     <span __boilernet_label="0" __num="27">
      A discussion
     </span>
    </a>
   </p>
   <p>
    <span __boilernet_label="1">
     Project management must decide when a product is ready torelease to end users. That decision is based on a mass ofinevitably imperfect information, including an estimate of theproduct's bugginess. The testing team creates that estimate. Thetest manager reports it, together with an assessment of howaccurate it is. If it's not accurate enough, because not enoughtesting has been done, the test manager must also report on whatthe testing team is doing to obtain better information. When willproject management know enough to make a reasoned ship decision?
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     The test manager has two responsibilities: to report the
    </span>
    <u>
     <span __boilernet_label="1">
      right
     </span>
    </u>
    <span __boilernet_label="1">
     information, and to report the information
    </span>
    <u>
     <span __boilernet_label="1">
      right
     </span>
    </u>
    <span __boilernet_label="1">
     . Both areequally important. If you don't have the right information, you'renot doing your job. If you don't report it so that it's accepted,valued, and acted upon, you might as well not be doing your job.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     To address both topics, this paper is organized around thesituation in which the test manager presents "the publicface" of the testing team: the status meeting. It covers theinformation to be presented and the needs of the people involved(which govern how to present that information usefully).
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="5">
     <b>
      <span __boilernet_label="1">
       1. Context
      </span>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     We assume a product developed in two distinct phases: featuredevelopment and product stabilization. We are specificallyconcerned with status meetings during stabilization, which iswhen the pressure is highest. During the stabilization phase,developers should do nothing but fix bugs. Ideally, they do notadd or change features.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Testers do three types of testing during stabilization:
    </span>
   </p>
   <ol>
    <li>
     <b>
      <span __boilernet_label="1">
       Planned testing.
      </span>
     </b>
     <span __boilernet_label="1">
      The tester is assigned some functional area or property of the product (such as the "printing subsystem" or "response to heavy network load"). At the beginning of the task, he knows the approach to take, what a "complete" set of tests would look like, and how much time is allocated. (This knowledge, like anything else in the project, is subject to revision, but it should be roughly correct.)
     </span>
    </li>
    <li>
     <b>
      <span __boilernet_label="1">
       Guerrilla testing.
      </span>
     </b>
     <span __boilernet_label="1">
      This is testing that opportunistically seeks to find severe bugs wherever they may be. Guerrilla testing is much less planned than the previous kind. The extent of planning might be: "Jane, you're the most experienced tester, and you have a knack for finding bugs. From now until the end of the project, bash away at whatever seem to be the shakiest parts of the product." Guerrilla tests are usually not documented or preserved. See [Kaner93] for more information.
     </span>
    </li>
    <li>
     <b>
      <span __boilernet_label="1">
       Regression testing.
      </span>
     </b>
     <span __boilernet_label="1">
      The tester reruns tests to see if one that used to pass now fails. If so, some change has broken the product.
     </span>
    </li>
   </ol>
   <p>
    <span __boilernet_label="1">
     In the first part of stabilization, planned tests dominate. Asstabilization proceeds, more and more regression testing is done.At the end of the project, the testing effort shifts entirely toregression testing and guerrilla testing.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     For more about the stabilization phase, see [Cusumano96] and [Kaner93].For various software development models, see [McConnell96].
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="5">
     <b>
      <span __boilernet_label="1">
       2. A Note on Numbers
      </span>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     Throughout the paper, we ask you to compare actual results toyour plan. For example, we recommend that you track whatproportion of their time your team spends rerunning tests vs. howmuch you expected they'd spend. A natural question is, "Well,then, what proportion
    </span>
    <u>
     <span __boilernet_label="1">
      should
     </span>
    </u>
    <span __boilernet_label="1">
     we expect?" We're notgoing to answer that question because the answers vary and, to beuseful, require project-specific context. On the other hand, wethink it's terrible that test managers have nowhere to go to getthose answers, other than to buttonhole their colleagues and swapwar stories. For that reason, we will try to collect a variety ofanswers and rules of thumb from experienced test managers and putthem on the web page http://www.stlabs.com/marick/root.htm.
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="5">
     <b>
      <span __boilernet_label="1">
       3. Questions toAnswer in the Status Meeting
      </span>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     During the project status meeting, you should be ready toanswer certain questions, both explicit and implicit.
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="4">
     <b>
      <i>
       <span __boilernet_label="1">
        3.1 When will webe ready to ship?
       </span>
      </i>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     During stabilization, successive builds are made, found not tobe good enough, and replaced. It's a stressful time because,early in stabilization, it often seems as if the product will
    </span>
    <u>
     <span __boilernet_label="1">
      never
     </span>
    </u>
    <span __boilernet_label="1">
     be good enough. There's a lot of pressure to show forwardprogress. Much attention revolves around a graph like this:
    </span>
   </p>
   <p>
   </p>
   <p align="center">
    <img height="324" src="image5.gif" width="338"/>
   </p>
   <p align="center">
   </p>
   <p>
    <span __boilernet_label="1">
     Everyone wants to know when those lines will cross, meaningall must-fix bugs have been fixed. Part of the answer comes fromknowing how fast open bugs will be resolved. But another part ofit comes from knowing how many new must-fix bugs can be expected.Our argument in this section is as follows:
    </span>
   </p>
   <ol>
    <li>
     <span __boilernet_label="1">
      Estimating this number is extremely difficult.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      If you don't do it, someone else will, probably worse than you would. So you should do it.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      Slight changes to the "intuitive way" of test planning and tracking can make the estimates better. So make those changes, as long as they don't get in the way of finding bugs.
     </span>
    </li>
   </ol>
   <p>
    <span __boilernet_label="1">
     To estimate this number, the natural tendency is to do somesort of curve-fitting (by using a ruler, a spreadsheet, or somemore sophisticated tool). If you do, your extrapolations willlikely be wildly wrong, because that curve is due to a changingmixture of completed tasks, tasks in progress, and tasks not yetbegun, each with a different rate of finding bugs. Better thanextrapolating from the summary graph is extrapolating from eachtesting task individually, using a spreadsheet like this:
    </span>
   </p>
   <p>
   </p>
   <p align="center">
    <img height="104" src="image6.gif" width="379"/>
   </p>
   <p align="center">
   </p>
   <p>
    <b>
     <span __boilernet_label="1">
      Important
     </span>
    </b>
   </p>
   <p>
    <span __boilernet_label="1">
     What's being estimated with this spreadsheet is the totalnumber of bugs
    </span>
    <i>
     <span __boilernet_label="1">
      this testing effort will find before it'sfinished
     </span>
    </i>
    <span __boilernet_label="1">
     . It is not the total number of bugs in the product.It is not the number of failures customers will see or report.Those would be better numbers to know, but this one isnevertheless useful.
    </span>
   </p>
   <p>
   </p>
   <p>
   </p>
   <p>
    <b>
     <span __boilernet_label="1">
      Comparing apples and oranges
     </span>
    </b>
   </p>
   <p>
    <span __boilernet_label="1">
     The spreadsheet above assumes that bugs will be found atroughly the same rate throughout a task. That's unlikely. Atypical graph of cumulative bugs discovered vs. working time (whichis more meaningful than calendar time) will look like this:
    </span>
   </p>
   <p align="center">
    <img height="209" src="image7.gif" width="290"/>
   </p>
   <p>
    <span __boilernet_label="1">
     The reason for the curve is that different activities arelumped together in one task. Early in the task, the tester islearning an area of the product and designing tests. While testdesign does find bugs, the rate of bug discovery is lower thanwhen the tests are run for the first time. So, when testexecution begins, the curve turns up. As testing proceeds, anincreasing amount of time will be spent in regression testing (especiallychecking bug fixes). Because rerunning a test is less likely tofind a bug than running it for the first time, the curve beginsto flatten out.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Your estimates will be more accurate if you track test design,first-time execution, and regression testing separately.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Notes:
    </span>
   </p>
   <ol>
    <li>
     <span __boilernet_label="1">
      We don't pretend that test execution doesn't have an element of design in it - while running tests, testers get ideas for new tests, which they then execute. But counting that "embedded design" as test execution should do no harm to what are already rough estimates. Major discoveries should make you schedule additional explicit tasks to be tracked separately.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      The beginning of test execution will probably find more bugs than the end, because it's common for testers to run the most promising tests first. Even if tests were run in random order, some bugs are obvious enough that many of the tests will find them. While this "front-loading" of bugs might skew your estimates, you certainly don't want to improve them by intentionally finding bugs later!
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      You might find it most useful not to track retesting of bug fixes. Instead, simply assume that some percentage of fixed bugs will cause new bugs that will later be found by retesting. Discover the percentage not by searching the literature (the numbers vary widely) but by reference to experience with your own project or other projects in your company.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      In guerrilla testing, in which there's no formal design nor much rerunning of tests, the bug discovery rate nevertheless usually follows an S-shaped pattern, if not a more sawtooth pattern as the tester switches from opportunity to opportunity:
     </span>
    </li>
   </ol>
   <p align="center">
    <img height="226" src="image8.gif" width="250"/>
   </p>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      A linear estimate early in the effort would guess high. That's not necessarily bad - not because it gives you more breathing room, but because the harm of estimating high is less than the harm of estimating low. You should temper the estimate with some judgment about when the curve is likely to level off.
     </span>
    </p>
   </blockquote>
   <p>
    <b>
     <span __boilernet_label="1">
      Reality Check
     </span>
    </b>
   </p>
   <p>
    <span __boilernet_label="1">
     You may object that this approach seems hopelessly idealistic.Your project is chaotic enough, with code that changes so much,and so many bugs, and so much churn in the features, and... thatthere's no hope of having tasks that stay the same long enoughfor extrapolations to be meaningful. If that's the case - if yourstabilization phase is anything but stable and you're reallydoing mostly guerrilla testing - there's no point in presentingwildly inaccurate estimates at the project status meeting.Concentrate on answering the other questions presented in thispaper.
    </span>
   </p>
   <p>
   </p>
   <p>
    <b>
     <span __boilernet_label="1">
      What about unstarted tasks?
     </span>
    </b>
   </p>
   <p>
    <span __boilernet_label="1">
     Here's a different spreadsheet, one for a project in whichconfiguration testing hasn't started yet:
    </span>
   </p>
   <p align="center">
    <img height="104" src="image9.gif" width="379"/>
   </p>
   <p>
   </p>
   <p>
    <span __boilernet_label="1">
     If you haven't started a task, you have no data from which toextrapolate. But you have to, because the project manager needsestimates in order to run the project. Here are some solutions:
    </span>
   </p>
   <ul>
    <li>
     <span __boilernet_label="1">
      Don't get into this situation. By the time the stabilization phase is well enough underway that this graph becomes the focus of attention, you should have made some progress on all testing tasks. That almost certainly means some context switching. For example, the tester responsible for the scenario testing task will have to stop part-way through and do some configuration testing. That's annoying, disruptive, and reduces efficiency - but only if you take a tester-centric view of the universe, one where getting testing done is the primary goal. Your team's primary goal is discovering useful information and making it available as early as possible. The project manager needs to know if there are configuration problems before the last minute.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      Extrapolate using numbers for testing that's already underway. For example, both "test printing" and "test editing" are functional tests of discrete subsystems. If the printing subsystem is half the size of the editing subsystem, and half as much testing is planned, you might reasonable predict half as many fatal bugs will be discovered. There are, of course, many variables that might confound that estimate: the programmer who wrote the printing code may be a much better programmer, the size metric you used (be it lines of code, number of branches, function points, programmer hours spent) may not capture the relative bugginess of the two subsystems, and so on. However, it's a better estimate than simply guessing, and it will improve rapidly as real testing begins.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      Extrapolate using numbers from past projects. We recommend that anyone doing any sort of data mining read up on the statistical subfield of exploratory data analysis. Two entertaining and influential early books are [Tukey77] and [Mosteller77].
     </span>
    </li>
   </ul>
   <p>
    <b>
     <span __boilernet_label="1">
      Reality check: you will certainly be wrong
     </span>
    </b>
   </p>
   <p>
    <span __boilernet_label="1">
     Even if you are careful with your extrapolation, you shouldrealize that the number calculated is certainly wrong. Attachingtoo much certainty to the numbers - and especially striving formeaningless precision - will lead to madness. Concentrate ontesting, and hope that the errors in your estimates roughlycancel each other out.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Ideally, you'd like to give your estimates in the form ofintervals, like this:
    </span>
   </p>
   <table border="1" bordercolor="#000000" cellpadding="7" cellspacing="1" width="469">
    <tbody>
     <tr>
      <td valign="top" width="19%">
       <font size="2">
        <span __boilernet_label="1">
         Week 1
        </span>
       </font>
      </td>
      <td valign="top" width="81%">
       <font size="2">
        <span __boilernet_label="1">
         "We predict 122 to 242 bugs. Best estimate is 182."
        </span>
       </font>
      </td>
     </tr>
     <tr>
      <td valign="top" width="19%">
       <font size="2">
        <span __boilernet_label="1">
         Week 2
        </span>
       </font>
      </td>
      <td valign="top" width="81%">
       <font size="2">
        <span __boilernet_label="1">
         "We predict 142 to 242 bugs. Best estimate is 192."
        </span>
       </font>
      </td>
     </tr>
     <tr>
      <td valign="top" width="19%">
       <font size="2">
        <span __boilernet_label="1">
         Week 3
        </span>
       </font>
      </td>
      <td valign="top" width="81%">
       <font size="2">
        <span __boilernet_label="1">
         "We predict 173 to 233 bugs. Best estimate is 203."
        </span>
       </font>
      </td>
     </tr>
     <tr>
      <td valign="top" width="19%">
       <font size="2">
        <span __boilernet_label="1">
         Week 4
        </span>
       </font>
      </td>
      <td valign="top" width="81%">
       <font size="2">
        <span __boilernet_label="1">
         "We predict 188 to 228 bugs. Best estimate is 208."
        </span>
       </font>
      </td>
     </tr>
     <tr>
      <td valign="top" width="19%">
       <font size="2">
        <span __boilernet_label="1">
         Week 5
        </span>
       </font>
      </td>
      <td valign="top" width="81%">
       <font size="2">
        <span __boilernet_label="1">
         "We predict 205 to 215 bugs. Best estimate is 210."
        </span>
       </font>
      </td>
     </tr>
    </tbody>
   </table>
   <p>
   </p>
   <p>
    <span __boilernet_label="1">
     Unfortunately, we expect your data will be unstable enoughthat your calculated "confidence intervals" will be sowide as to be pointless. We're also unaware of statisticaltechniques that apply to this particular problem. If you wish toexplore related notions, see the large body of literature onsoftware reliability and software reliability growth ([Musa87][Lyu96]).Your best bet might be using historical project data. If yourcompany keeps good records of past projects, and has a reasonablyconsistent process, you may be able to use errors in historicalestimates to make predictions for this project.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     We think that, rather than straining for numbers far moreexact than anything else in the project, you should concentrateon understanding and explaining what caused any recent changes inthe estimates. Here, your goal is confidence - in you and yourteam - not confidence intervals. You want to say things like:
    </span>
   </p>
   <dir>
    <li>
     <span __boilernet_label="1">
      "Last week's prediction was roughly 69 more must-fix bugs. However, Dawn noticed some oddities as she continued the testing of the modules from our outsource partners, NightFly Technologies. She beefed up her test plan with some targeted use case tests. That led to last week's jump in bugs found, and it raised our prediction to roughly 95 bugs."
     </span>
    </li>
   </dir>
   <p>
    <span __boilernet_label="1">
     The description behind the numbers is
    </span>
    <i>
     <span __boilernet_label="1">
      much
     </span>
    </i>
    <span __boilernet_label="1">
     more usefulthan the numbers, because it helps the project manager makecontingency plans.
    </span>
   </p>
   <p>
    <b>
     <span __boilernet_label="1">
      Final notes on estimation
     </span>
    </b>
   </p>
   <ol>
    <li>
     <span __boilernet_label="1">
      Toward the end of the stabilization phase, your estimates will become useless. That's the point at which your team is concentrating on regression tests for the last few bug fixes, together with guerrilla testing that tries to squeeze out a few more must-fix bugs. This period is inherently unpredictable - a single lingering bug could take a month to fix. It's the worst part of the project. You will still have provided a valuable service by helping to predict when the hellish period starts.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      [DeMarco82] has useful things to say about estimation. [Bach94] describes a bug tracking system initially used to produce estimates of the sort described here. See also the description of Bach's work in [Keuffel94], which says, "Bach now believes too many independent variables exist to make useful predictions of ship dates by simply leveraging off the critical-bug database." [Keuffel95] continues the description with some interesting deductions you can make from the shape of a project-wide graph.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      Make sure that the graph of found vs. fixed "must fix" bugs doesn't dominate attention to the exclusion of other shipping criteria. Other criteria that are sometimes used include trends in number of bugs found, active, fixed, and verified; number of lower-severity unfixed bugs; changes in bug severity distribution; and amount of recent change to the source code ([Cusumano95], [Kaner93], [Rothman96]). And the product shouldn't ship until testing is finished, including regression testing and a last-chance guerrilla test of the final build.
     </span>
    </li>
   </ol>
   <p>
    <font face="Arial,HELVETICA" size="4">
     <b>
      <i>
       <span __boilernet_label="1">
        3.2 Which bugsshould be fixed?
       </span>
      </i>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     A discussion of individual bugs might happen in the projectmeeting or in a separate bug classification meeting. (This mightbe a discussion of all bugs or, more productively, a discussionof only controversial bugs.) You are likely to be the person whoprints and distributes the summaries and detailed listings of newand unresolved bugs. It's preferable if the project manager leadsthe meeting, not you. First, he or she has the ultimateresponsibility. Second, you will be an active participant, and it'sdifficult to do a good job as both participant and meeting leader.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Here are some key points:
    </span>
   </p>
   <ol>
    <li>
     <span __boilernet_label="1">
      Developers have a psychological interest in assigning bugs a low priority or deferring them. However, you may well tend to over-emphasize the severity of a bug, which is just as harmful. The best advocates for fixing a bug are the people who will have to live with its consequences. That's not you. It's more likely to be customer service people and marketing people. They should be invited to bug classification meetings. Your role as advocate is to make sure that the bug report is understood correctly. If the bug report is being misinterpreted (for example, as being an unlikely special case instead of a symptom of a more general problem), correct the misinterpretation.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      The key issue in such a meeting is determining whether the risk of fixing a bug exceeds the risk of shipping it with the product. When helping assess risk, your job is to provide two bits of informed opinion:
     </span>
    </li>
   </ol>
   <blockquote>
    <ul>
     <li>
      <span __boilernet_label="1">
       What are the testing implications of fixing the bug (whether it's fixed alone or batched with other bugs in the same subsystem)? How much will adequately testing the fix cost? Do you have the resources to do it? Is it a matter of rerunning regression tests? (What's the backlog?) Or will new tests have to be written?
      </span>
     </li>
     <li>
      <span __boilernet_label="1">
       What's the rate of regressions? How often do bug fixes fail? (If the rate of regressions is high, especially in the subsystem in question, and that subsystem has light test coverage, the risk of a bugfix is high.)
      </span>
     </li>
    </ul>
   </blockquote>
   <ol>
    <li>
     <span __boilernet_label="1">
      Higher management may be tracking open bug counts. That can lead to unproductive pressure to make the numbers look good at the expense of fixing fewer but more important bugs. (See [Kaner93], chapter six.) As a responsible project member, you should be alert to decisions that provide only internal benefit, not benefit to the customer.
     </span>
    </li>
   </ol>
   <p>
    <span __boilernet_label="1">
     We urge you to read the discussions of bugs and bug reportingin [Bach94] and [Kaner93].
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="4">
     <b>
      <i>
       <span __boilernet_label="1">
        3.3 Where are thedanger areas in the project?
       </span>
      </i>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     As part of the estimation process, you will become aware of"hot spots" in the product: areas where there areunusually many bugs. In the charts above, testing editing isfinding far more bugs than any other testing task.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     The project manager is sometimes the last to know about hotspots, since developers are incorrigibly optimistic. They tendnot to admit (to themselves) that a particular subsystem is outof control until too late. It's your job as a test manager toreport on hot spots at the status meeting. You need to do thiswith care, lest your team be viewed as the enemy by development.Here are some tips.
    </span>
   </p>
   <ul>
    <li>
     <b>
      <span __boilernet_label="1">
       Make sure it really is a hot spot.
      </span>
     </b>
    </li>
   </ul>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      The heat of a hot spot is relative to the size and complexity of the area under test. If the editing subsystem is four times as complex as the printing subsystem, four times as many bugs is not surprising. You should have complexity information available, because you should have used it in test planning.
     </span>
    </p>
   </blockquote>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      Historical information will help to keep hot spots in perspective. What's the bug rate for previous projects (per line of code, function point, or some other measure of size)? What parts of this project are out of line with the historical averages?
     </span>
    </p>
   </blockquote>
   <ul>
    <li>
     <b>
      <span __boilernet_label="1">
       Report on product, not people.
      </span>
     </b>
    </li>
   </ul>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      With perhaps a few exceptions, such as configuration testing, the hot spots you find will be associated with a particular person or team. Don't criticize them. Maybe they've done a shoddy job, but don't assume you have enough information to make that judgement. Even if you do, it's counterproductive at this point.
     </span>
    </p>
   </blockquote>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      Not only must you not criticize the developers, it's important that they not think you're criticizing them. Your manner should be calm, dispassionate, and professional. Use "we" instead of "you". You should take care to say something good along with the bad news:
     </span>
    </p>
   </blockquote>
   <blockquote>
    <blockquote>
     <p>
      <span __boilernet_label="1">
       "That last overhaul of the account management module has really streamlined the user interface. Paul, the tester, says that it takes half as many mouse moves to get through a test as before. Of course, that means that users will see the same speedup. Unfortunately, his tests have found a big jump in the number of bugs, to the point where we're well above average for modules in this product."
      </span>
     </p>
    </blockquote>
   </blockquote>
   <ul>
    <li>
     <b>
      <span __boilernet_label="1">
       Warn of hot spots in advance.
      </span>
     </b>
    </li>
   </ul>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      Do not announce a hot spot without prior warning. You know you've damaged your usefulness when the project manager explodes, "What do you mean, the networking subsystem is in crisis! Why haven't I heard about this before?!"
     </span>
    </p>
   </blockquote>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      One reason for no prior warning is that there's no data. Some testing task hadn't started until the past week. When it did start, the tester involved found so many bugs she had to down tools right away. Avoid the issue by doing at least a little testing on all tasks as soon as possible. You can defuse the issue by reporting, at each status meeting, which important tasks haven't started, along with a prediction of when each will start.
     </span>
    </p>
   </blockquote>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      Another reason for a sudden announcement is that a testing task suddenly crosses the threshold where you feel you should raise the alarm. Avoid this by announcing which tasks you're watching more carefully, though they don't yet merit the designation "hot spot".
     </span>
    </p>
   </blockquote>
   <ul>
    <li>
     <b>
      <span __boilernet_label="1">
       Track hot spots carefully.
      </span>
     </b>
    </li>
   </ul>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      The response to a hot spot should be some development effort (redesign or rework). When that's complete, report on the success: "We reran all the regression tests on the new build, with unusually few regressions, which personally gives me a warm and fuzzy feeling. We've started executing new tests on the module, and things look good so far - many fewer bugs than in the last version. I think we're out of the woods on this one."
     </span>
    </p>
   </blockquote>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      An alternative to rework is to "debug the code into working". In that case, you need to adjust your plans. Whereas before you scheduled a fixed amount of testing, you now have an open-ended testing task. You might increase the number of designed tests; you will certainly devote more time to guerrilla testing. You need to continue testing until you stop discovering a disproportionately large number of bugs. You will look for a graph like this:
     </span>
    </p>
   </blockquote>
   <p align="center">
    <img height="204" src="image10.gif" width="263"/>
   </p>
   <blockquote>
    <p>
     <span __boilernet_label="1">
      After nearly twice as much testing as planned, you seem to have exhausted the bugs in the code.
     </span>
    </p>
   </blockquote>
   <p>
    <b>
     <span __boilernet_label="1">
      Another type of hot spot
     </span>
    </b>
   </p>
   <p>
    <span __boilernet_label="1">
     In addition to total bugs, track "churn": how manyregression tests fail. If a disproportionately high number of bugfixes or other changes break what used to work, there's a problem.The subsystem is likely fragile and deserves rework or a greatertesting effort. Here's a chart that might cause alarm:
    </span>
   </p>
   <p align="center">
    <img height="237" src="image11.gif" width="360"/>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="4">
     <b>
      <i>
       <span __boilernet_label="1">
        3.4 Are youworking smart?
       </span>
      </i>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     Any buggy area announced late in the project is going to bestressful. It will likely mean some upheaval, some shifting ofdevelopment priorities. As test manager, you must also shift yourpriorities, both because you now know more about risky areas andalso because you must be seen as doing more than smuglyannouncing a crisis and letting someone else deal with it. It'simportant to both be helpful and also be seen to be helpful.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Display graphs to show that the testing team is flexiblyadapting what you do to what you discover. One such graph follows.It breaks testing down by task. It plots effectiveness (bugsfound) behind measures of effort to date. That way, mismatchesbetween effort and result are visually striking. The graph usestwo measures of effort: time spent and
    </span>
    <i>
     <span __boilernet_label="1">
      coverage
     </span>
    </i>
    <span __boilernet_label="1">
     achieved.Code coverage measures how thoroughly tests exercise the code.That correlates roughly with thoroughness of testing. The mostcommon type of code coverage measures which lines of code havebeen executed. For other types of coverage, see [Kaner96] and [Marick95].For a discussion of how coverage can be misused, see [Marick97].
    </span>
   </p>
   <p>
    <img height="604" src="image12.gif" width="711"/>
   </p>
   <p>
    <span __boilernet_label="1">
     Here's what you might say in the project status meeting. (Noticethat the individual testing activities tracked separately for bugcount estimation are here grouped into larger tasks.)
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     "The stress tests and db tests are paying off in terms ofbugs found. The stress tests are only 14% complete, so you canexpect a lot of stress-type bugs in the next few weeks. You'llnotice that we early in the project cut back on the number ofstress tests - although we're 14% complete against plan, that'sonly
    </span>
    <i>
     <span __boilernet_label="1">
      three percent
     </span>
    </i>
    <span __boilernet_label="1">
     of what we originally hoped to do. Ifstress testing keeps finding bugs, we'll up the amount of stresstesting planned.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     "The db tests are 55% complete against plan, and they'vegot 80% coverage. Given the type of testing we're doing, we mightsee the bug finding rate start leveling off soon. We may not doall the testing planned.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     "The GUI tests have been a washout. We're not findingbugs. The code seems solid. We're stopping that testing now andshifting the tester onto network testing, where we're findingtons of bugs. In the network testing, you'll note that thecoverage is high in comparison to the number of tests written.That's because we've finished the "normal use tests".The error handling tests, which we couldn't start until thenetwork simulator was ready, will add less coverage, but weexpect them to be a good source of bugs.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     "The calc tests were a botch. We're nearly done, withoutmuch to show for it. My fault - I let the schedule get away fromme. We're stopping testing right away."
    </span>
   </p>
   <p>
    <b>
     <span __boilernet_label="1">
      Comparing yourself to others
     </span>
    </b>
   </p>
   <p>
    <span __boilernet_label="1">
     It's a common developer prejudice that testers write a lot oftests, but it's wasted motion because they're not finding theimportant bugs. First, the test manager needs to check whetherthe prejudice is justified. (All too often, it is.) Second, if it'snot, reassure the project team. Doing both things starts with agraph like this, which shows what percentage of bugs are beingfound by different types of planned testing vs. those found bychance.
    </span>
   </p>
   <p align="center">
    <img height="244" src="image13.gif" width="632"/>
   </p>
   <p>
    <span __boilernet_label="1">
     This shows a fairly reasonable breakdown. Most new bugs arefound by the testing team: newly written feature tests,regression tests run again, guerrilla tests, and plannedconfiguration tests. 10% of new bugs reported are due toregressions, which is not outlandish. Beta customers arereporting configuration bugs at a good clip, which is what youexpect from them. (You might want to consider doing more in-houseconfiguration testing if the total number of configuration bugsis unusually high.) There are a reasonable number of bugs fromother sources (beta bugs that are not configuration problems,bugs found by developers and other in-house use, and so on).
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     This graph is more alarming:
    </span>
   </p>
   <p align="center">
    <img height="248" src="image14.gif" width="634"/>
   </p>
   <p>
    <span __boilernet_label="1">
     Too many bugs are being found by "other", not enoughby the testing team. They're being discovered by chance. Can thetesting effort change to detect them more reliably? The fact thatguerrilla testing is doing so much better than the new featuretests is also evidence that something's wrong with the plannedtesting effort. Perhaps the problem is that the project is "churning":there are too many regressions. Maybe almost everyone on thetesting team is spending so much time running regression teststhat they have little time to execute new ones (except for a fewvery productive guerrilla testers).
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     When many bugs have been found, the testing team can become abottleneck. Report clearly when regression testing load exceedsthe plan. Such reports, made early and forthrightly, leave theimpression that slips in the testing schedule are a result ofpitching in to help the project. Done late, you may get the blamefor the final slips, slips that happen as developers wait for youto finish up your testing.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     When reporting on your schedule, remind everyone that theearliest possible ship date is the time required to complete allplanned tests, plus the time required to rerun all repeatabletests (both manual and automated) on the final build, and timefor a series of guerrilla tests on that build.
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="4">
     <b>
      <i>
       <span __boilernet_label="1">
        3.5 What does yourwork mean?
       </span>
      </i>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     We've seen how you can predict the number of bugs testing willfind and use graphs to reallocate the testing effortappropriately. But what does all this effort mean?
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Recall that the project manager wants to reduce uncertaintyand risk. He worries that this product will be an embarrassmentor worse after it ships. Knowing how many bugs testing will findis useful, because it lets him predict whether the product canship on time, but what he
    </span>
    <i>
     <span __boilernet_label="1">
      really
     </span>
    </i>
    <span __boilernet_label="1">
     wants to know is how manybugs testing
    </span>
    <i>
     <span __boilernet_label="1">
      won't
     </span>
    </i>
    <span __boilernet_label="1">
     find.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     Let's assume that you graph the bugs found by
    </span>
    <u>
     <span __boilernet_label="1">
      new
     </span>
    </u>
    <span __boilernet_label="1">
     (notrerun) designed tests for some particular testing task. If thattask is consistent throughout (you don't apply different testingtechniques during different parts of the task, or save thetrickiest tests for the end), you'd expect a graph that lookssomething like this:
    </span>
   </p>
   <p align="center">
    <img height="224" src="image15.gif" width="272"/>
   </p>
   <p>
    <span __boilernet_label="1">
     You'd expect something of a flurry of new bugs at first. Manyof those are the bugs that are easy to find, so that manydifferent tests would hit them. After those have been disposed of,there's a more-or-less steady stream of new bugs. (In practice,the curve might tail down more sharply, since there's a tendencyto concentrate on the most productive tests first, in order toget bugs to the developers quickly.)
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     This graph doesn't really let you predict what happens in thefield. You can't simply extrapolate the curve. If you did aperfect job of test design, you've tried all the bug-revealingcases, and all bugs have been found. If your test cases are veryunrepresentative of typical customer use, you might have missedmany of the bugs customers will find, in which case there will bea surge of new bug reports when the product is released (notunusual).
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     So what can you do? You have to extrapolate from previousprojects.
    </span>
   </p>
   <ul>
    <li>
     <span __boilernet_label="1">
      If you discovered 70% of the configuration bugs during test in release 3.0, and if you continue to perform configuration testing roughly the same way, the safest bet is that you'll again miss 30% of the bugs.
     </span>
    </li>
    <li>
     <span __boilernet_label="1">
      If you consistently apply the same set of testing techniques, and you size the testing effort against the product in a consistent way, you should hope that you'll find the same proportion of bugs from effort to effort. In practice, of course, the variability among testers "doing the same thing" is as large as it is for programmers (that is to say, enormous). You can reduce variability by having testers review each other's test designs, but you won't eliminate it. That's OK. Saying "based on the past three projects, we predict between 50 and 120 more must-fix bugs will be found by customers" beats only being able to say "We tested hard, and we hope we didn't miss too much."
     </span>
    </li>
   </ul>
   <p>
    <span __boilernet_label="1">
     When reporting extrapolations, don't give raw numbers. Say,"we predict the reliability of this product will be typical[or high, or low] for this company." If possible, makecomparisons to specific products: "This product did aboutaverage on the planned tests, but remarkably few bugs were foundin guerrilla testing or regression testing. The last product withthat pattern was FuzzBuster 2.0, which did quite well in thefield. We predict the same for this one."
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     The accuracy of your predictions depends on a consistentprocess. Consistent processes are the subject of industrialquality control, especially the work of Shewhart, Deming, Juran,and others. We in software can learn much from them, so long aswe're careful not to draw unthinking analogies. [Deming82] isoften cited, but the style is somewhat telegraphic. [Aguayo91]has been recommended to one of us (Marick), who first learnedabout statistical quality control from a draft of [Devor91]. TheSoftware Engineering Institute's Capability Maturity Model can bethought of as an adaptation of modern industrial quality controlto software, though the coverage of testing is sketchy. See [Humphry89]or [Curtis95].
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="5">
     <b>
      <span __boilernet_label="1">
       4. Summary of thepaper
      </span>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     As the frequent bearer of bad news, the test manager has adifficult job. The job is easier if he or she appears competent,knowledgeable, and proactive. It's a matter of having therelevant information, presenting it well, being able to answerreasonable questions. A goal should be to walk out of the statusmeeting knowing that the testing team and its work have beenrepresented well.
    </span>
   </p>
   <p>
    <span __boilernet_label="1">
     As the test manager, you are the keeper of data that can helpyou understand trends and special occurrences in the project,provided you avoid the two "data traps": havingunjustified faith in numbers, and rejecting numbers completelybecause they're imperfect. Take a balanced view and use data as aspringboard to understanding.
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="5">
     <b>
      <span __boilernet_label="1">
       5. Acknowledgements
      </span>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="1">
     Keith Davis made helpful comments on an earlier draft of thispaper.
    </span>
   </p>
   <p>
    <font face="Arial,HELVETICA" size="5">
     <b>
      <span __boilernet_label="1">
       6. References
      </span>
     </b>
    </font>
   </p>
   <p>
    <span __boilernet_label="0" __num="177">
     [Aguayo91]
    </span>
    <br/>
    <span __boilernet_label="0" __num="178">
     Rafael Aguayo,
    </span>
    <i>
     <span __boilernet_label="0" __num="179">
      Dr. Deming,
     </span>
    </i>
    <span __boilernet_label="0" __num="180">
     Fireside / Simon and Schuster,1991.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="181">
     [Bach94]
    </span>
    <br/>
    <span __boilernet_label="0" __num="182">
     James Bach, "Process Evolution in a Mad World," in
    </span>
    <i>
     <span __boilernet_label="0" __num="183">
      Proceedingsof the Seventh International Quality Week
     </span>
    </i>
    <span __boilernet_label="0" __num="184">
     , (Software Research,San Francisco, CA), 1994.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="185">
     [Curtis95]
    </span>
    <br/>
    <span __boilernet_label="0" __num="186">
     Mark C. Paulk, Charles V. Weber, and Bill Curtis (ed),
    </span>
    <i>
     <span __boilernet_label="0" __num="187">
      TheCapability Maturity Model: Guidelines for Improving the SoftwareProcess
     </span>
    </i>
    <span __boilernet_label="0" __num="188">
     , Addison-Wesley, 1995.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="189">
     [Cusumano95]
    </span>
    <br/>
    <span __boilernet_label="0" __num="190">
     M. Cusumano and R. Selby,
    </span>
    <i>
     <span __boilernet_label="0" __num="191">
      Microsoft Secrets,
     </span>
    </i>
    <span __boilernet_label="0" __num="192">
     Free Press,1995.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="193">
     [DeMarco82]
    </span>
    <br/>
    <span __boilernet_label="0" __num="194">
     Tom DeMarco,
    </span>
    <i>
     <span __boilernet_label="0" __num="195">
      Controlling Software Projects: Management,Measurement, and Estimation
     </span>
    </i>
    <span __boilernet_label="0" __num="196">
     , Prentice Hall, 1982.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="197">
     [Deming82]
    </span>
    <br/>
    <span __boilernet_label="0" __num="198">
     J. Edwards Deming,
    </span>
    <i>
     <span __boilernet_label="0" __num="199">
      Out of the Crisis,
     </span>
    </i>
    <span __boilernet_label="0" __num="200">
     MIT Center forAdvanced Engineering Study, 1982.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="201">
     [Devor92]
    </span>
    <br/>
    <span __boilernet_label="0" __num="202">
     Richard E. Devor, Tsong-How Chang, and John W. Sutherland
    </span>
    <i>
     <span __boilernet_label="0" __num="203">
      ,Statistical Quality Design and Control: Contemporary Concepts andMethods
     </span>
    </i>
    <span __boilernet_label="0" __num="204">
     , MacMillan, 1992.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="205">
     [Humphrey89]
    </span>
    <br/>
    <span __boilernet_label="0" __num="206">
     Watts Humphrey,
    </span>
    <i>
     <span __boilernet_label="0" __num="207">
      Managing the Software Process,
     </span>
    </i>
    <span __boilernet_label="0" __num="208">
     Addison-Wesley,1989.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="209">
     [Kaner93]
    </span>
    <br/>
    <span __boilernet_label="0" __num="210">
     C. Kaner, J. Falk, and H.Q. Nguyen,
    </span>
    <i>
     <span __boilernet_label="0" __num="211">
      Testing Computer Software(2/e),
     </span>
    </i>
    <span __boilernet_label="0" __num="212">
     Van Nostrand Reinhold, 1993.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="213">
     [Kaner96]
    </span>
    <br/>
    <span __boilernet_label="0" __num="214">
     Cem Kaner, "
    </span>
    <a href="http://www.kaner.com/coverage.htm">
     <span __boilernet_label="0" __num="215">
      SoftwareNegligence &amp; Testing Coverage
     </span>
    </a>
    <span __boilernet_label="0" __num="216">
     ," in
    </span>
    <i>
     <span __boilernet_label="0" __num="217">
      Proceedings ofSTAR 96
     </span>
    </i>
    <span __boilernet_label="0" __num="218">
     , (Software Quality Engineering, Jacksonville, FL),1996. ()
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="219">
     [Keuffel94]
    </span>
    <br/>
    <span __boilernet_label="0" __num="220">
     Warren Keuffel, "James Bach: Making Metrics Fly at Borland,"
    </span>
    <i>
     <span __boilernet_label="0" __num="221">
      Software Development
     </span>
    </i>
    <span __boilernet_label="0" __num="222">
     , December 1994.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="223">
     [Keuffel94]
    </span>
    <br/>
    <span __boilernet_label="0" __num="224">
     Warren Keuffel, "Further Metrics Flights at Borland,"
    </span>
    <i>
     <span __boilernet_label="0" __num="225">
      SoftwareDevelopment
     </span>
    </i>
    <span __boilernet_label="0" __num="226">
     , January 1995.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="227">
     [Lyu96]
    </span>
    <br/>
    <span __boilernet_label="0" __num="228">
     Michael R. Lyu (ed.),
    </span>
    <i>
     <span __boilernet_label="0" __num="229">
      Handbook of Software ReliabilityEngineering,
     </span>
    </i>
    <span __boilernet_label="0" __num="230">
     McGraw-Hill, 1996.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="231">
     [Marick95]
    </span>
    <br/>
    <span __boilernet_label="0" __num="232">
     Brian Marick,
    </span>
    <i>
     <span __boilernet_label="0" __num="233">
      The Craft of Software Testing,
     </span>
    </i>
    <span __boilernet_label="0" __num="234">
     Prentice Hall,1995.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="235">
     [Marick97]
    </span>
    <br/>
    <span __boilernet_label="0" __num="236">
     Brian Marick, "
    </span>
    <a href="../classic/mistakes.html">
     <span __boilernet_label="0" __num="237">
      ClassicTesting Mistakes
     </span>
    </a>
    <span __boilernet_label="0" __num="238">
     ," in
    </span>
    <i>
     <span __boilernet_label="0" __num="239">
      Proceedings of STAR 97
     </span>
    </i>
    <span __boilernet_label="0" __num="240">
     , (SoftwareQuality Engineering, Jacksonville, FL), 1997.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="241">
     [McConnell96]
    </span>
    <br/>
    <span __boilernet_label="0" __num="242">
     Steve McConnell,
    </span>
    <i>
     <span __boilernet_label="0" __num="243">
      Rapid Development,
     </span>
    </i>
    <span __boilernet_label="0" __num="244">
     Microsoft Press, 1996.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="245">
     [Mosteller77]
    </span>
    <br/>
    <span __boilernet_label="0" __num="246">
     Frederick Mosteller and John W. Tukey,
    </span>
    <i>
     <span __boilernet_label="0" __num="247">
      Data Analysis andRegression,
     </span>
    </i>
    <span __boilernet_label="0" __num="248">
     Addison-Wesley, 1977.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="249">
     [Musa87]
    </span>
    <br/>
    <span __boilernet_label="0" __num="250">
     J. Musa, A. Iannino, and K. Okumoto,
    </span>
    <i>
     <span __boilernet_label="0" __num="251">
      Software Reliability :Measurement, Prediction, Application
     </span>
    </i>
    <span __boilernet_label="0" __num="252">
     , McGraw-Hill, 1987.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="253">
     [Rothman96]
    </span>
    <br/>
    <span __boilernet_label="0" __num="254">
     Johanna Rothman, "
    </span>
    <a href="http://world.std.com/~jr/Papers/QW96.html">
     <span __boilernet_label="0" __num="255">
      Measurements toReduce Risk in Product Ship Decisions
     </span>
    </a>
    <span __boilernet_label="0" __num="256">
     ," in
    </span>
    <i>
     <span __boilernet_label="0" __num="257">
      Proceedingsof the Ninth International Quality Week
     </span>
    </i>
    <span __boilernet_label="0" __num="258">
     , (Software Research,San Francisco, CA), 1996.
    </span>
   </p>
   <p>
    <span __boilernet_label="0" __num="259">
     [Tukey77]
    </span>
    <br/>
    <span __boilernet_label="0" __num="260">
     John W. Tukey,
    </span>
    <i>
     <span __boilernet_label="0" __num="261">
      Exploratory Data Analysis,
     </span>
    </i>
    <span __boilernet_label="0" __num="262">
     Addison-Wesley,1977.
    </span>
   </p>
   <table bgcolor="#CCCCCC" border="0" cellspacing="0" width="100%">
    <tbody>
     <tr>
      <td width="11%">
       <p align="center">
        <a href="/services.html" title="Consulting, training, and contracting services">
         <span __boilernet_label="0" __num="263">
          Services
         </span>
        </a>
       </p>
      </td>
      <td width="11%">
       <p align="center">
        <a href="/writings.html" title="Papers, essays, and book reviews">
         <span __boilernet_label="0" __num="264">
          Writings
         </span>
        </a>
       </p>
      </td>
      <td width="9%">
       <p align="center">
        <a href="/tools.html" title="Freeware tools I have written">
         <span __boilernet_label="0" __num="265">
          Tools
         </span>
        </a>
       </p>
      </td>
      <td width="17%">
       <p align="center">
        <a href="/agile/index.html" title="Agile Testing">
         <span __boilernet_label="0" __num="266">
          Agile Testing
         </span>
        </a>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p>
   </p>
   <p align="center">
    <span __boilernet_label="0" __num="267">
     Comments to
    </span>
    <a href="mailto:marick@testing.com">
     <span __boilernet_label="0" __num="268">
      marick@testing.com
     </span>
    </a>
   </p>
  </text>
 </body>
</html>